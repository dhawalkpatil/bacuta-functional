\documentclass[oneside]{book}
%\includeonly{title,def,biblio,abstract}
% \setlength{\oddsidemargin}{0.0in}
% \setlength{\textwidth}{6.5in}
% \setlength{\baselineskip}{19pt}
% \setlength{\parskip}{19pt}
% \pagestyle{empty}
% \setlength{\topmargin}{-0.4in}
% \setlength{\textheight}{8.7in}
\usepackage{epsfig}
\usepackage{amsmath,amssymb,amsthm, graphicx, enumitem, mathrsfs, mathtools, fancyhdr, breqn}

%\usepackage{showkeys}
\newcommand{\beqn}{\begin{equation}}
\newcommand{\eeqn}{\end{equation}}
\newcommand{\nn}{\nonumber}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{proposition}{Proposition}
\newtheorem{example}{Example}
\newtheorem{cons}{Consequence}
\newcommand{\R}{{\mathbb R}}
\newcommand{\RC}{\mathcal{R}}
\newcommand{\C}{{\mathbb C}}
\newcommand{\Q}{{\mathbb Q}}
\def\K{\mathbb{K}}
\newcommand{\T}{{\mathbb T}}
\newcommand{\lm}{\lambda}
\newcommand{\Z}{{\mathbb Z}}
\newcommand{\N}{{\mathbb N}}
\newcommand{\I}{{\mathbb I}}
\newcommand{\F}{{\mathscr F}}
\newcommand{\G}{{\mathcal G}}
\newcommand{\M}{\mathscr{M}}
\newcommand{\B}{\mathscr{B}}
\newcommand{\A}{\mathscr{A}}
\newcommand{\PS}{\mathscr{P}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\Mod}{\ (\text{mod}\ #1)}
\usepackage{showkeys}
%\usepackage{comment}
\newcommand{\p}{\partial}
\newcommand{\pp}{\prime\prime}
\newcommand{\pr}{\prime}
\newcommand{\al}{\alpha}
\newcommand{\be}{\beta}
\newcommand{\ga}{\gamma}
\newcommand{\na}{\nabla}
\newcommand{\De}{\Delta}
\newcommand{\ep}{\epsilon}
\newcommand{\Om}{\Omega}
\newcommand{\fd}{{\text{finite-dimensional}}}
\newcommand{\la}{\langle}
\newcommand{\ra}{\rangle}
\newcommand{\de}{\delta}
\newcommand{\si}{\sigma}
\newcommand{\vr}{\varrho}
\newcommand{\vp}{\varphi}
\newcommand{\rhp}{\rightharpoonup}
\newcommand{\hra}{\hookrightarrow }
\DeclareMathOperator{\meas}{meas\,}
\DeclareMathOperator{\supp}{supp\,}
\DeclareMathOperator{\esup}{ess\,sup}
\DeclareMathOperator{\einf}{ess\,inf}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\range}{range}
\newcommand{\pom}{\partial\Omega}
\newcommand{\suml}{\sum\limits}
\newcommand{\sumi}{\sum\limits_{n = 1}^{\infty}}
\newcommand{\sumk}{\sum\limits_{k = 1}^{\infty}}
\newcommand{\sumkn}{\sum\limits_{k = 1}^{n}}
\newcommand{\sumj}{\sum\limits_{j = 1}^{\infty}}
\newcommand{\sumo}{\sum\limits_{n = 0}^{\infty}}
\newcommand{\dlim}{\displaystyle\lim}
\newcommand{\dint}{\displaystyle\int}
\newcommand{\dintab}{\dint_a^b}
\newcommand{\dintinf}{\dint_{-\infty}^{\infty}}
\newcommand{\dintpi}{\dint_{-\pi}^{\pi}}
\newcommand{\dintc}{\dint_{0}^{2\pi}}
\newcommand{\dintopi}{\dint_{0}^{\pi}}
\newcommand{\dintio}{\dint_{0}^{\infty}}
\newcommand{\dintl}{\dint\limits}
\newcommand{\dintr}{\dint_{\R}}
\newcommand{\dintom}{\dint_{\Om}}
\newcommand{\dintbr}{\dint_{\p R}}
\newcommand{\dints}{\dint_{\sigma}}
\newcommand{\dsum}{\displaystyle\sum}
\newcommand{\bom}{\mbox{\boldmath$\omega$}}
\newcommand{\Bom}{\mbox{\boldmath$\Omega$}}
\newcommand{\sgn}{\mbox{${\rm sgn}\,$}}
\newcommand{\ov}{\overline}
\newcommand{\un}{\underline}
\newcommand{\ml}{\mathbf{L}}
\newcommand{\dotp}{\boldsymbol{\cdot}}
\newcommand{\rint}{\mathcal{R}}
\newcommand{\CS}{\mathscr{C}}
\newcommand{\FF}{\mathscr{F}}
\newcommand{\bv}{\Bigm\vert}
\newcommand{\ms}{m^{\star}}
\newcommand{\sbs}{\subset}
\newcommand{\bi}{b^i}
\newcommand{\sse}{\subseteq}
\newcommand{\bcl}{\bigcup\limits}
\newcommand{\bcln}{\bigcup\limits_{n=1}^{\infty}}
\newcommand{\bclj}{\bigcup\limits_{j=1}^{\infty}}
\newcommand{\bclk}{\bigcup\limits_{k=1}^{\infty}}
\newcommand{\bckn}{\bigcup\limits_{k=1}^{n}}
\newcommand{\bcap}{\bigcap\limits_{k=1}^{n}}
\newcommand{\bcapl}{\bigcap\limits}
\newcommand{\bcapi}{\bigcap\limits_{n=1}^{\infty}}
\newcommand{\bcapj}{\bigcap\limits_{j=1}^{\infty}}
\newcommand{\limi}{\lim\limits_{n \to \infty}}
\newcommand{\lims}{\limsup\limits_{n \to \infty}}
\newcommand{\fp}{f_{+}}
\newcommand{\fne}{f_-}
\newcommand{\bs}{\backslash}
\newcommand{\dintg}{\displaystyle\int\limits_{\gamma}}
\newcommand{\cif}{\dfrac{1}{2\pi i} \dint_{\sigma} \dfrac{f(w)}{w - z} \, dw}
\newcommand{\cifn}{\dfrac{1}{2\pi i} \dint_{\sigma} \dfrac{f_n(w)}{w - z} \, dw}
\newcommand{\zar}{|z - a| < R}
\newcommand{\zare}{|z-a| = R}
\newcommand{\zarf}{\hspace{5mm} |z - a| < R}
\newcommand{\zapf}{\hspace{5mm} |z-a| < \rho}
\newcommand{\zap}{|z - a| < \rho}
\newcommand{\zape}{|z-a| = \rho}
\newcommand{\raz}{|z-a| > R}
\newcommand{\razf}{\hspace{5mm} |z - a| > R}
\newcommand{\rar}{R_1 < |z-a| < R_2}
\newcommand{\lar}{\sum\limits_{n = -\infty}^{\infty}}
\newcommand{\sumu}{\sum\limits_{n = -\infty}^{-1}}
\newcommand{\sumijd}{\sum\limits_{i, j = 1}^{d}}
\newcommand{\sumid}{\sum\limits_{i = 1}^{d}}
\newcommand{\res}{\text{Res}}
\newcommand{\vn}{\varnothing}
\newcommand{\aij}{a^{ij}}
\newcommand{\lp}{\left(}
\newcommand{\rp}{\right)}
\newcommand{\rb}{\right]}
\newcommand{\lb}{\left[}
\newcommand{\llb}{\left\lbrace}
\newcommand{\rrb}{\right\rbrace}
\newcommand{\vtf}{{\vspace{-25pt}}}
\newcommand{\sa}{\sigma\text{-algebra}}
\newcommand{\mx}{\mathbf{x}}
\newcommand{\md}{\mathbf{d}}
\newcommand{\mn}{\mathbf{n}}
\newcommand{\my}{\mathbf{y}}
\newcommand{\hx}{\hat{\mx}}
\newcommand{\mxi}{\mathbf{\xi}}
\newcommand{\bo}{\mathcal{O}}
\newcommand{\norm}{\| \cdot \|}
\newcommand{\xs}{(x_n)_{n \geq 1} }
\newcommand{\Xs}{X^{\star}}
\newcommand{\Xss}{X^{\star \star}}
\newcommand{\Hss}{H^{\star \star}}
\newcommand{\xst}{x^{\star}}
\newcommand{\Vs}{V^{\star}}
\newcommand{\Ts}{T^{\star}}
\newcommand{\Ks}{K^{\star}}
\newcommand{\Tss}{T^{\star \star}}
\newcommand{\Ys}{Y^{\star}}
\newcommand{\As}{A^{\star}}
\newcommand{\Hs}{H^{\star}}
\newcommand{\Yss}{Y^{\star \star}}
\newcommand{\ys}{y^{\star}}
\newcommand{\yss}{y^{\star \star}}
\newcommand{\zs}{z^{\star}}
\newcommand{\zss}{z^{\star \star}}
\newcommand{\xss}{x^{\star \star}}
\newcommand{\Hz}{H_0^1}
\newcommand{\Hzo}{H_0^1(\Om)}
\newcommand{\Hzod}{\left( H_0^1(\Om)\right)^{\star}}
\newcommand{\mlt}{\ml^2}
\newcommand{\mlto}{\ml^2(\Om)}
\DeclareMathOperator*{\spa}{span}
\DeclareMathOperator*{\esssup}{ess\,sup}
\newcommand{\ns}{(X, \| \cdot \|)}
\newcommand{\weak}{\rightharpoonup}
\newcommand{\weaks}{\stackrel{\ast}{\rightharpoonup}}
\newcommand{\Vp}{V^{\perp}}
\newcommand{\Vpp}{V^{\perp \perp}}
\newcommand{\inner}{(\cdot, \cdot)}
\newcommand{\ips}{\text{inner product space}}
\newcommand{\Ai}{A^{-1}}
\newcommand{\saj}{\text{self-adjoint}}
\newcommand{\prp}{\prime \prime}
\newcommand{\is}{i^{\star}}
\makeatletter
\def\moverlay{\mathpalette\mov@rlay}
\def\mov@rlay#1#2{\leavevmode\vtop{%
   \baselineskip\z@skip \lineskiplimit-\maxdimen
   \ialign{\hfil$\m@th#1##$\hfil\cr#2\crcr}}}
\newcommand{\charfusion}[3][\mathord]{
    #1{\ifx#1\mathop\vphantom{#2}\fi
        \mathpalette\mov@rlay{#2\cr#3}
      }
    \ifx#1\mathop\expandafter\displaylimits\fi}
\makeatother
\newcommand{\cupdot}{\charfusion[\mathbin]{\cup}{\cdot}}
%

\begin{document}

\pagestyle{myheadings}


{\bf \large
\begin{center}
806 - Functional Analysis \\
Fall 2017 - University of Delaware \\
Notes by Dr. Constantin Bacuta, Transcribed by Nicholas Russell
\end{center}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%

\noindent
\input{banach}
\section*{Finite-Dimensional Normed Spaces}
\setcounter{equation}{0}
\begin{lemma}
Let $X$ be an $N$-dimensional normed space over $\K$. Let $\{ u_1, \cdots, u_N\}$ be a basis for $X$. For any $\alpha \in \R^N$, $\alpha = (\alpha_1, \alpha_2, \cdots, \alpha_N)$, define 
\[ T\alpha = \alpha_1 u_1 + \alpha_2 u_2 + \cdots + \alpha_N u_N \in X  \]
(Note that $T: \R^N \mapsto X$). Then, we have that 
\begin{enumerate}
\item $T$ is invertible and linear
\item $T$ is continuous
\item There exists $c_1, c_2 > 0$ such that 
\begin{equation}
c_1 \| \alpha \| \leq \| T \alpha \| \leq c_2 \| \alpha \|
\end{equation} 
for all $\alpha \in \R^N$.
\end{enumerate}
Consequently, $T^{-1}$ is continuous and 
\begin{equation}
c_1 \| T^{-1}x \| \leq \| x \| \leq c_2 \| T^{-1}x \| 
\end{equation}
for all $x \in X$ (i.e. $T$ is an isomorphism).
\end{lemma}
\begin{proof}
For 1, we know that $T\alpha = 0$ implies that $\alpha = 0$ since $\alpha = \alpha_1 u_1 + \cdots + \alpha_N u_N$ and $\{u_1, u_2, \cdots u_N\}$ is a basis must imply that $\alpha_1 = \cdots = \alpha_N = 0$. Therefore, this implies that $T$ is invertible. \\
For 3, we can see that 
\begin{align*}
\| T \alpha \| & = \| \alpha u_1 + \alpha_2  u_2 + \cdots + \alpha_N u_N \| \\
& \leq |\alpha_1| \| u_1 \| + \cdots + |\alpha_N| \| u_N \| \\
& \leq \sqrt{\alpha_1^2 + \cdots + \alpha_N^2} \cdot \sqrt{\| u_1 \|^2 + \cdots + \| u_N \|^2}\\
& \| \alpha \| c_2,
\end{align*}
for all $\alpha \in \R^N$. This also proves 2 as well since we have boundedness. Now, to prove that we can have a $c_1$ on the other side, we assume by contradiction that $(1)$ does not hold. Thus for any $n \geq 1$, (take $c_1 = \frac{1}{n}$), there must exist an $\alpha^n \in \R^n$ such that 
\[ \| T \alpha^n \| < \frac{1}{n} \| \alpha^n \|.\]
Now, take $\beta^n = \frac{\alpha^n}{\| \alpha^n \|} \in \R^N$ and thus, $\| \beta^n \| = 1$. Note now that 
\[ \| T \beta^n \| = \frac{1}{\| \alpha^n \| } \| T \alpha^n \| < \frac{1}{\| \alpha^n \|} \frac{1}{n} \| \alpha^n \| = \frac{1}{n}.\]
This means that $T \beta^n \to 0$. Therefore, since $\beta^n \in \overline{B}(0, 1) \sbs \R^N,$ there must exist a convergent subsequence $\beta^{n_k}$. Therefore, $\beta^{n_k} \to \beta$ in $\R^N$. But, we have that $\| \beta^{n_k} \| = 1$ which implies that $\| \beta \| = 1$. Since $T$ is continupus, and $\beta^{n_k} \to \beta$, this must imply that $T\beta^{n_k} \to T \beta = 0$, which is a contradiction since $\| \beta \| = 1$ means that $\beta \neq 0$. This proves that $T$ is not injective, but this is a contradiction with 2. Thus, 3 must hold. \\
To prove the rest, all one does is denote $T\alpha = x$ and thus, $\alpha = T^{-1} x$ and it follows quickly. 
\end{proof}
\begin{definition}
If $\| \cdot \|_1$ and $\| \cdot \|_2$ are 2 norms on $X$, then they are \textbf{equivalent} if there exists $\alpha, \beta > 0$ such that
\[ \alpha \| x \|_1 \leq \| x \|_2 \leq \beta \| x \|_1\]
for all $x\in X$. 
\end{definition}
\begin{cons}
In any $\fd$ normed space, all norms are equivalent.
\end{cons}
\begin{proof}
Assume that $\dim(X) = N$ and $u_1, \cdots, u_N$ is a basis. Denote $T: \R^N \to X$ as in the above lemma. Then, $(\alpha_1, \cdots, \alpha_N)$ implies that $\alpha_1 u_1 + \cdots + \alpha_N u_N \in X$. Then, if we have $(X, \| \cdot \|_1)$, then the lemma tells us that $a_1, a_2$ such that 
\[a_1 \| T^{-1} x \|_{\R^N} \leq \| x \|_1 \leq a_2 \| T^{-1} x \|_{\R^N}\]
for all $x \in X$. As well, if we have $(X, \| \cdot \|_2)$, we know that there exist $b_1, b_2 $ such that 
\[ b_1 \| T^{-1} x \|_{\R^N} \leq \| x \|_2 \leq b_2 \| T^{-1} x \|_{\R^N}\]
for all $x \in X$. Therefore, we can see that 
\[ \frac{b_1}{a_2} \| x \|_1 \leq \| x \|_2 \leq \frac{b_2}{a_1} \| x \|_1\]
which proves that they are equivalent.
\end{proof}
\begin{cons}
Any $\fd$ normed space is complete. 
\end{cons}
\begin{proof}
Let $(X, \| \cdot \| )$ of dimension $N$. Then, from the lemma, we know that there exist $C_1, C_2$ such that
\begin{equation}
C_1 \| T^{-1} x \| \leq \| x \| \leq C_2 \| T^{-1}x \|.
\end{equation}
Let $(x_n)$ be a Cauchy sequence in $X$. Then, from (3), we get that 
\[ \| T^{-1}x_n - T^{-1} x_m \| \leq \frac{1}{C_2} \| x_n - x_m \| \to 0 \]
as $m, n \to 0$. Then we see that $(T^{-1} x_n)_{n \geq 1} \in \R^N$ is a Cauchy sequence. Since $\R^N$ is complete, we must have that $(T^{-1} x_n)_{n \geq 1} $ has a limit, say $\beta$ as $n \to \infty$. Then, since $T$ is continuous, we have that $T(T^{-1} x_n) = x_n \to T\beta \in X$. Therefore, $(x_n)$ is convergent in $X$ and thus, it is complete. 
\end{proof}
\begin{cons}
Let $(X, \| \cdot \|)$ be a normed space and $B_1 = \overline{B}(0, 1)$. Then, $B_1$ is compact if and only if $X$ is finite dimensional. 
\end{cons}
\begin{proof}
First, assume that $X$ is finite dimensional. Then, using the Lemma, $T: \R^n \to X$ is an isomorphism. Since $B_1$ is closed and bounded and $T^{-1}$ is continuous, then $T^{-1}(B_1)$ is closed and bounded in $\R^n$. By Bolzano-Weierstrass, $T^{-1}(B_1)$ must be compact. Since $B_1 = T(T^{-1}(B_1)$, then we must have that $B_1$ is compact. \\
For the other direction, we will need to two steps:
\begin{enumerate}[topsep=-15pt, itemsep=0pt]
\item[(i)] We will need the Riesz Lemma: Suppose $Y \subsetneq X$ closed subspace of $X$, which is a normed space. Given $\epsilon >0$, there exists an $x \in X$ with $\| x \| = 1$ and $d(x, Y) = \inf\limits_{y \in Y} \| x - y \| \geq 1 - \epsilon$. 
\item[(ii)] We will prove that contrapositive of the statement, which is the following: If $X$ is infinite dimensional, then $B_1$ is not compact. 
\end{enumerate}
To do this, we assume that $X$ is infinite dimensional. We pick $e_1 \in X$ such that $\| e_1 \| = 1$ and let $E_1 = \text{span}\{ e_1 \}$. Then, $E_1$ is finite dimensional, thus $E_1$ is complete, and therefore, $E_1$ is closed. Since $E_1 \subsetneq X$, by Riesz Lemma when $\epsilon = \frac{1}{2}$, there exists an $e_2 \in X$ such that $\| e_2 \| = 1$ and $\| e_2 - e_1 \| > \frac{1}{2}$. Now, we define $E_2 = \spa\{ e_1, e_2\} \subsetneq X$ and it is a closed subspace. Thus, by induction, we can contruct $E_n = \spa\{ e_1, e_2, \cdots, e_n \}$ and $\| e_k \| = 1$ for all $k \in \{ 1, \cdots, n \}$ and $\| e_j - e_k \| \geq \frac{1}{2}$ for all $j \neq k$. Then, $E_n \sbs B_1$ but has no convergent subsequence and therefore, we have that $B_1$ is not compact. 
\end{proof}
\subsection*{Hahn-Banach Theorem, Consequences, and Representation of the Dual}
We assume that $X$ is a normed space over $\K$. We define 
\[ \Xs = \B(X, \K) = \{ f: X \mapsto \K : f \text{ linear and bounded } \}\]
and we call $\Xs$ the dual of $X$. Then, if $f \in \Xs$, we have that 
\[ \| f \| = \| f \|_{\Xs} = \sup\limits_{\| x \| \leq 1} |f(x)| = \sup\limits_{\| x \| \neq 0 } \frac{|f(x)|}{\| x \|}.\]
Then, $(\Xs, \| \cdot \|_{\Xs})$ is a Banach space. \\
\indent For an example, let $X = \R^n$. For any $a \in \R^n$, define a real functional, $f_a$. Then, $f_a(x) = a_1 x_1 + \cdots + a_n x_n$ and thus, 
\[\| f_a \| = \sup \frac{|f_a(x)|}{\| x \|} = \| a \| = \sqrt{a_1^2 + \cdots + a_n^2}\]
We note that $f_a(x) = 0$ implies that $a_1 x_1 + \cdots + a_n x_n = 0$. Thus, we can see that we can represent the dual of $\R^n$ as $\R^n$, or $\left( \R^n \right)^{\star} = \R^n$. 
\begin{definition}
Assume that $X$ is a real normed space. Then, $P: X \mapsto \R$ is a convex functional if 
\begin{enumerate}
\item[(i)] $p(x + y) \leq p(x) + p(y) $ for all $x, y \in X$;
\item[(ii)] $p(tx) = tp(x)$ for all $t \in [0, \infty)$ and for all $x \in X$.
\end{enumerate}
\end{definition}
We note that any convex functional must be a convex function.
\begin{theorem}\textbf{(Hahn-Banach)} 
Let $X$ be a normed space over $\R$ and let $P: X \mapsto \R$ be a convex functional on $X$. Consider $V \sse X$ subspace and let $f: V \mapsto \R$ be a linear functional such that $f(x) \leq p(x)$ for all $x \in V$. Then, there exists a linear functional $F: X \mapsto \R$ such that 
\begin{enumerate}
\item[(i)] $F(x) = f(x)$ for all $x \in V$;
\item[(ii)] $F(x) \leq p(x)$ for all $x \in X$. 
\end{enumerate}
\end{theorem}
\begin{proof}
Proof is in the textbook, but the goal is to first extend to $V + \spa\{ x_0 \}$ where $x_0 \not\in V$. Then, we can use the Housdorf Maximum Principle to get the result on $X$. 
\end{proof}
\setcounter{cons}{0}
We now will discuss some different consequences of the Hahn-Banach theorem, now starting with the Extension Theorems.
\begin{theorem} \textbf{(Extension Theorem: Real Case)}
Assume that $V \sbs X$ real normed subspace and $f: V\mapsto \R$ is a linear, bounded functional. Then, there exists an $F: X \mapsto \R$ functional such that $F(x) = f(x)$ for all $x \in V$ and $\| F \|_{\Xs} = \| f \|_{V^{\star}}$.
\end{theorem}
\begin{proof}
Let $M = \| f \|_{V^{\star}}$ and define $p: X \mapsto \R$, where $p = m\| x \|$. Note that $p$ is a convex functional and $|f(x)| = \| f \|_{\Vs} \| x \| = m \| x \| = p(x)$ for all $x \in V$. Then by H-B theorem, there exists an $F: X \mapsto \R$ that extends $F$ to $X$ and $F(x) \leq p(x)$ for all $x \in X$. Now, we need to prove that $F$ is bounded and $\| F \|_{\Xs} = \| f \|_{\Vs}$. To do this, we note that $F(x) \leq m \| x \|$ for all $ x \in X$ and we have that $-F(x) = F(-x) \leq m \| -x \|\leq m \| x \|$ for all $x \in X$. Therefore, this implies that $|F(x)| \leq m \| x \|$ for all $x \in X$ and thus, $F$ is bounded ($F \in \Xs$) and $\| F \| \leq \|f \|$. But, 
\[ \| F \| = \sup\limits_{, x \in X, \| x \| \leq 1} |F(x)| \geq \sup\limits_{, x \in V, \| x \| \leq 1} |F(x)| = \sup\limits_{ x \in V, \| x \| \leq 1} |f(x)|  = \| f \|.\]
Therefore, $\| F \|_{\Xs} = \| f \|_{\Vs}$. 
\end{proof}
\begin{theorem} \textbf{(Extension Theorem: Complex Case)}
Assume that $V \sbs X$ complex normed subspace and $f: V\mapsto \C$ is a linear, bounded functional. Then, there exists an $F: X \mapsto \C$ functional such that $F(x) = f(x)$ for all $x \in V$ and $\| F \|_{\Xs} = \| f \|_{V^{\star}}$.
\end{theorem}
\begin{proof}
Let $X_{\R}$ be $X$ considered over a vector space of $\R$. Let $u: V_{\R} \mapsto \R$, $u(x) = \Re(f(x))$. Then, 
\[ |u(x)|_{X_{\R}^{\star}} = |\Re(f(x))| \leq |f(x)| \leq \| f \|_{\Vs} \cdot \| x \|\]
for all $x \in X_{\R}$. Then, $u \in X_{\Re}^{\star}$ and $\| u \|_{X_{\Re}^{\star}} \leq \| f \|_{\Vs}$. If we apply the real case for $u: V_{\R} \mapsto \R$, then there exists $u: X_{\Re} \mapsto \R$ that is linear and extends $u$ and 
\[ \| u \|_{X_{\Re}^{\star}} = \| u \|_{V_{\Re}^{\star}} \leq \| f \|_{\Vs}.\]
Define $F(x) = u(x) - i u(ix)$ for all $x \in X$. Then, this map satisfies all of the requirements, $F$ extends $f$ and $F$ is linear, continuous, and bounded. As well, $\| F \|_{\Xs} = \| f \|_{\Vs}$.
\end{proof}
\begin{proposition}\textbf{(Separation of Points)} If $(X, \| \cdot \|)$ is a normed space and $x \neq y$, then there exists an $F \in \Xs$ such that $F(x) \neq F(y)$.
\end{proposition}
\begin{proof}
Let $V = \spa\{ x - y\}$ supposing that $x \neq y$. Then, define $f: V \mapsto \K$ where $f(t(x-y)) = t$ for all $t \in \K$. Note that $f(x-y) = 1$, which implies that $f(x) - f(y) = 1$ and thus, $f(x) \neq f(y)$. Now, we also note that 
\[\frac{|f(t(x-y))|}{\| t(x-y)\|} = \frac{|t|}{|t| \| x - y \|} = \frac{1}{\| x - y\|} > 0 \]
for all $x \neq y \in X$. Therefore, 
\[ \| f \|_{\Vs} = \sup\limits_{\| f \| \neq 0} \frac{\| f(t(x-y))\|}{t \| x - y\|} = \frac{1}{\| x- y\|},\]
which implies that $f$ is bounded. Thus, by the extension theorem, there exists an $F: X \mapsto \K$ such that $F(z) = f(z)$ for all $z \in X$ and $\| F\| = \| f \| = \frac{1}{\| x - y\|} < \infty$. Therefore, $F \in \Xs$ and $F(x) \neq F(y)$. 
\end{proof}
We note that if $(X, \| \cdot \|)$ is a normed space, Proposition 1 has the following conseuqences:
\begin{enumerate}
\item[(i)] $\phi(x) = \phi(y)$ for all $\phi \in \Xs$ implies that $x = y$;
\item[(ii)] $\phi(x) = 0$ for all $x \in \Xs$ implies that $x = 0$;
\item[(iii)] For all $x \in X$, $x \neq 0$, there exists a $\phi \in \Xs$ such that $\phi(x) \neq 0$.
\end{enumerate}
\begin{proposition}
Let $\ns$ be a Banach space and $x_0 \in X$, $x_0 \neq 0$. Then, there exists a $\phi \in \Xs$ such that $\phi(x_0) = \| x_0 \|$ and $\| \phi \| = 1$. 
\end{proposition}
\begin{proposition} \textbf{(Separating a point from a convex set)} Let $\ns$ be a real normed space. If $\Om \sbs X$ is an open, convex set containing $\mathbf{0}$ and $x_0 \not\in \Om$, then there exists an $F \in \Xs$ such that $F(x) < 1 \leq F(x_0)$ for all $x \in \Om$.
\end{proposition}
\begin{proof}
Define $p: X \mapsto .
\end{cases}\]
From this, we see that $a < b$ implies that 
\[g_b(x) - g_a(x) = \begin{cases}
1, & x \in [a, b] \\
0, & \text{ otherwise. }
\end{cases} \]
Therefore, for all $\epsilon > 0$, there exists a $g_{\epsilon}$ piece wise constant such that $\| g - g_{\epsilon } \|_{\ml^2} < \epsilon$. Then, if we take 
\[ |(f_n, g) - (f, g)| = |(f_n - f, g)| = |(f_n - f, g - g_{\epsilon}) + (f_n - f, g_{\epsilon})| \leq |(f_n - f, g - g_{\epsilon}) | + |(f_n - f, g_{\epsilon})| < \epsilon \]
for large $n$. Then, we can have weak convergence. \\
\indent Then, we have three claims to finish the proof:
\begin{enumerate}
\item[Claim 1:] $f_n \not\to f$ since $\| f_n - f \|^2 \frac{1}{8}$ for all $n \geq 1$. 
\item[Claim 2:] $T: \ml^2([0, \pi]) \mapsto \ml^2([0,\pi])$, where 
\[ (Tg)(x) = \dint_0^{x} g(t) \, dt\]
is a compact operator. We can prove this using the Arzela-Ascoli Theorem
\item[Claim 3:] $T f_n \to Tf$ uniformly on $[0, \pi]$ and hence $Tf_n \to Tf$ strongly in $\ml^2([0, \pi])$. We see that 
\[ (Tf_n)(x) = \dint_0^x \sin^2(nt) \, dt = \frac{x}{2} - \frac{1}{4n} \sin(2nx)\]
for all $x\in [0, \pi]$. As well, $(Tf)(x) = \frac{x}{2}$. Therefore, we have that 
\[ |(Tf_n(x) - Tf(x)| = \frac{1}{4n} |\sin(2nx)| \leq \frac{1}{4n}\]
for all $x\in [0,\pi]$, $n \geq 1$. So, we have uniform convergence. Therefore, 
\[ \dint_0^{\pi} (Tf_n - Tf)^2 \, dx \leq \dint_0^{\pi} \frac{1}{16n^2} \to 0 \]
as $n \to \infty$. Therefore, we have strong convergence. 
\end{enumerate}
\subsection*{5.6 - Positive Definite Operators and Lax-Milgram Theorem}
\setcounter{equation}{0}
Let $H$ be a Hilbert space over $\R$ (all of this is true also over $\C$, but to simplify calculations, we only use $\R$). Let $A: H \mapsto H$.
\begin{definition}
We say that $A$ is \textbf{(strictly) positive definite} (SPD) if 
\begin{equation}
(Au, u) \geq \beta \| u \|^2, \hspace{3mm} \forall u \in H
\end{equation}
for some $beta \in \R$. 
\end{definition}
We note that if $H = \R^n$ and $A$ is an $n \times n$ matrix, then (1) implies that $Ax = 0$ implies that $x = 0$. Thus, $A$ is injective and thus, $A$ is invertible. 
\begin{theorem}
(\textbf{An SPD operator is invertible}) Let $A: H \mapsto H$ satisfying (1). Then, 
\begin{enumerate}
\item[(i)] For all $f \in H$, there exists a unique $u \in H$ such that $Au = f$ ($u = A^{-1} f$);
\item[(ii)] The inverse satisfies $\| A^{-1} \| \leq \frac{1}{\beta}$. 
\end{enumerate}
\end{theorem}
\begin{proof}
Let $A: H \mapsto H$ be a linear, bounded operator that satisfies (1). Then, from (1), we get that 
\[ \beta \| u \|^2 \leq (Au, u) \leq \| Au \| \| u \| \]
which implies that 
\begin{equation}
\| Au \| \geq \beta \| u \|, \hspace{3mm} \forall u \in H
\end{equation}
Therefore, we have that $A$ is injective and the range of $A$ is closed. Now, we claim that $\range(A) = H$. If this is untrue, then there exists a $w \in \range(A)^{\perp}$ such that $w \neq 0$. Then, take $u = w$ in (1), we get that $\beta \| w \| \leq (Aw, w) = 0$. Since $Aw \in \range(A)$ and $w \in \range(A)^{\perp}$, we get that $\| w \| = 0$, which implies that $w = 0$. Therefore, $A$ is onto and thus invertible, and (i) is proven. For (ii), first note that $\Ai$ is also linear and by taking $u = \Ai f$, from (2) we get 
\[ \beta \| \Ai f\| \leq \| f \|, \hspace{3mm} \forall f \in H.\]
Therefore, we get that $\| \Ai \| \leq \frac{1}{\beta}.$
\end{proof}
Let $H$ be a Hilbert space over $\R$ and let $a : H \times H \mapsto \R$ be a \textbf{bilinear form}, i.e.
\[ a(\alpha_1 u_1+ \alpha_2 u_2 , v) = \alpha_1 a(u_1, v) + \alpha_2 a(u_2, v)\]
\[ a(u, \beta_1 v_1 + \beta_2 v_2) = \beta_1 a(u, v_1) + \beta_2 a(u, v_2)\]
for all $\alpha_1, \beta_1, \alpha_2, \beta_2 \in \R$ and $v_1, v_2, u_1,u_2 \in H$. 
\begin{theorem}(\textbf{Lax-Milgram}) Assume that a is a bilinear form, $a: H \times H \mapsto \R$, and satisfies
\begin{enumerate}
\item[(i)] $|a(u, v)| \leq M \| u \| \| v \|$ for all $u, v \in H$ and for some $M > 0$ ($a$ is bounded) 
\item[(ii)] there exists a $\beta  > 0$ such that $a(u, u) \geq \beta \| u \|^2$ for all $u \in H$ ($a$ is coercive). 
\end{enumerate}
Let $F: H \mapsto \R$ be a bounded linear functional. Then, the problem 
\begin{center}
\textit{Find $u \in H$ such that $a(u, v) = F(v)$ for all $v \in H$,}
\end{center}
has a unique solution, $u \in H$, and $\| u \| \leq \frac{1}{\beta} \| F \|$. 
\end{theorem}
\begin{proof}
Since $G$, which maps $v \mapsto a(u, v)$ is linear and bounded (from (i)) on $H$, by the Riesz Representation Theorem, there exists a unique $u \in H$ such that 
\begin{equation}
G(v) = a(u, v) = (Au, v), \hspace{3mm} \forall v \in H.
\end{equation}
Using linearity of $a(\cdot, \cdot)$ with respect to the first component, one can prove that $A$ is a linear operator. Remembering that we can write 
\[ \| y \| = \sup\limits_{\| x \| \neq 0} \frac{|(x, y)|}{\| x \|}\]
for all $y \in H$, we have that 
\[ \| Au \| = \sup\limits_{\| v \| \neq 0} \frac{|a(u, v)|}{\| v \|} = \sup\limits_{\| v \| \neq 0} \leq \sup\limits_{\| v \| \neq 0} \frac{M \| u \| \| v \|}{\| v \|} = M \| u \|,\]
where $M \geq 0$ and $u \in H$. From (ii), the coercivity condition and (3), we get that $(Au, u) = a(u, u) \geq \beta \| u \|^2$. By using the Riesz Representation Theorem for $F$ we can find an $f \in H$ such that $F(v) = (f, v)$ for all $v \in H$. From the previous theorem, the problem $Au = f$ has a unique solution, $u = \Ai f$, and $\| u \| \leq \frac{1}{\beta} \| f\| = \frac{1}{\beta} \| F \|$. Since $Au = f$, we have that $(Au, v) = (f, v)$ for all $v \in H$, and thus, $a(u, v) = F(v)$ for all $v \in H$. 
\end{proof}
\section*{Chapter 6 - Compact Operators on Hilbert Spaces}
\setcounter{theorem}{0}
 \setcounter{proposition}{0}
  \setcounter{definition}{0}
 \setcounter{corollary}{0}
 \setcounter{cons}{0}
 \setcounter{equation}{0}
 \setcounter{lemma}{0}
If $A: \R^n \mapsto \R^n$, $A = (a_ij)_{i, j = \ov{1, n}}$., then 
\[ Ax = \left( \sum\limits_{j =1}^n a_{ij} x_j \right)_{i = \ov{1, n}} \in \R^n, \text{ where } x = \left[ \begin{matrix}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{matrix}\right]\]
We know that 
\begin{enumerate}
\item[(R1)] $A$ injective if and only if $A$ surjective. 
\item[(R2)] $\dim(\ker(A)) = \dim(\ker(A^{T}))$ and $\range(A)^{\perp} = \ker(A^T)$.
\end{enumerate}
Now, what happens if we replace $\R^n$ by $H$, an infinite dimensional Hilbert space? We will see that results (R1) and (R2) are still valid for $A = I - K$ if $K$ is compact. 
\subsection*{6.1 - Fredholm Alternative}
We now present information from previous chapters to remind ourselves of key facts that we will use going forward.
\begin{proposition}
\begin{enumerate}
\item[(1)] If $T \in \B(X, Y)$ where $X, Y$ are Hilbert spaces, then $\Ts: Y \mapsto X$ is the Hilbert Adjoint and $(Tx, y) = (x, \Ts y)$. 
\item[(2)] $\Tss = T$ and $\| \Ts \| = \| T \|$. 
\end{enumerate}
\end{proposition}
\begin{lemma}
For any $T \in \B(X, Y)$ where $X, Y$ are Hilbert we have that 
\begin{enumerate}
\item[(i)] $\ker(T) = \left( \range(\Ts) \right)^{\perp}$
\item[(ii)] $ \ker(\Ts) = \left( \range(T) \right)^{\perp}$.
\end{enumerate}
\end{lemma}
We note that $(ii)$ does not imply that $\range(T) = \ker(\Ts)$. This is because we have that
\[ \ker(T)^{\perp} = \range(T)^{\perp \perp} = \ov{\range(T)}. \]
So, the result is only true if $\range(T)$ is closed. 
\begin{theorem}(\textbf{Fredholm's Theorem)} Let $H$ be a Hilbert Space and let $K: H \mapsto H$ be a compact operator. Then, 
\begin{enumerate}
\item[(i)] $\ker(I - K)$ is finite dimensional
\item[(ii)] $\range(I - K)$ is closed
\item[(iii)] $\range(I - K) = \ker(I - \Ks)^{\perp}$
\item[(iv)] $\ker(I - K) = \{ 0 \} \Longleftrightarrow \range(I - K) = H$
\item[(v)] $\ker(I - K)$ and $\ker(I - \Ks)$ ($= \range(I - K)^{\perp}$) have the same dimension 
\end{enumerate}
\end{theorem}
\begin{proof}
\begin{enumerate}
\item[(i)] Assume that $\ker(I - K)$ is infinite dimensional. Let $\{ e_1, e_2, \cdots, e_n, \cdots \}$ be an orthonormal sequence in $\ker(I - K)$. Then $K e_n = e_n$ and $\| e_n \| = 1$, and $\| Ke_n - K e_m \| = \| e_n - e_m \| = 2$. But this implies that $(Ke_n)_{n \geq 1}$ does not admit a convergent subsequence, which contradicts $K$ being compact. 
\item[(ii)] We proved this for Banach spaces (HW \#3, Problem 9). Let $N = \ker(I - K) \sbs H$ which is closed. So, we have that $H = N \oplus N^{\perp}$. Therefore, for all $x \in H$, we have that $x = x_N + x_{N^{\perp}}$, where $x_N \in N$ and $x_{N^{\perp}} \in N^{\perp}$. So, $(I - K)x = (I - K) x_{N^{\perp}}$. So, $\range(I - K) = \range(I - K) \vert_{N^{\perp}}$. We claim that there exists a $c > 0$ such that 
\begin{equation}
\| (I - K) x \| \geq c \| x \|, \hspace{3mm} \forall x\in N^{\perp}.
\end{equation} 
To prove this, assume that (1) does not hold. Then, there exists an $(x_n)_{n \geq 1} \sbs N^{\perp}$ such that $\| x_n \| = 1$ and $\| (I - K) x_n \| \geq \frac{1}{n}$ for all $n \geq 1$. Therefore, we see that 
\begin{equation}
(I - K) x_n \to 0 \hspace{3mm} as n \to \infty.
\end{equation} 
Since $K$ is compact, $Kx_n$ must admit a convergent subsequence $(Kx_{n_j})_{ j \geq 1}$ which converges. From (2), we have that $(x_{n_j} - K x_{n_j})_{j \geq 1}$ must converge. Because two sequences converging implies the sum of the sequences converges, we have that $x_{n_j} - Kx_{n_j} +K x_{n_j} = x_{n_j}$ converges. But, since $(x_{n_j}) \sbs N^{\perp}$, which is closed, we have that $x_{n_j} \to x \in N^{\perp}$. Since $K$ is compact, we must have that $K$ is continuous. Thus, $Kx_{n_j} \to Kx$, and thus, $x_{n_j} - Kx_{n_j} \to x - Kx = (I- K) x = 0$ from (2). Therefore, $x in N$. But, $x \in N \cap N^{\perp}$ implies that $x = 0$, which contradicts the fact that $\| x_{n_j} \| = 1$. Therefore, the claim is proven and that implies that $I - K$ is surjective. 
\item[(iii)] Apply Lemma 1, part (ii) for $T = I - K$. Then, $\ker(I - \Ks) = \range(I - K)^{\perp}$. Then, 
\[ \ker(I - \Ks)^{\perp} = \range(I - K)^{\perp \perp} = \ov{\range(I - K)} = \range(I - K). \]
\item[(iv)] $(\rightarrow)$ Assume that $\ker(I - K) = \{ 0 \}$ and $\range(I - K) \neq H$. Define $H_1 = (I - K)H = \range(I - K) \subsetneq H$. Then, apply (ii) for the operator $(I - K) \vert_{H_1}$ to get that $H_2 = (I - K) H_1 = (I - K)^2 H$, and by injecivity, $(I - K) H = H$, but we assumed that this is not true. Therefore, we have that $H_2 \subsetneq H_1$. By induction, we find that for any $n \geq 1$, $H_n = (I - K)^n H$ is a closed subspace of $H$ and $H \supsetneq H_1 \supsetneq H_2 \supsetneq \cdots \supsetneq H_n \supsetneq \cdots$. Choose $(e_n) \sbs H_n \cap H_{n + 1}^{\perp}$ such that $\| phi \| = 1$. We claim that $Ke_n$ does not admit a convergent subsequence, and in fact for any $m < n$, $\| K e_n - K e_m \| \geq 1$. To this end, we see that 
\[ Ke_m K e_n = - \underbrace{(I - K) e_m}_{\in H_{m+1}}+ \underbrace{(I - K) e_n}_{\in H_{n+1} \rightarrow \in H_{m+1}} + e_m - \underbrace{e_n}_{\in H_{m+1}}.\]
Therefore, $K e_m - Ke_n = e_m + z_{m + 1}$, where $z_{m + 1} \in H_{m + 1}$. Therefore, 
\[ \| K e_m - K e_n \|^2 = \| e_m \|^2 + \| z_{m + 1}\|^2 \geq \| e_m \|^2 = 1.\]
Therefore, $(Ke_m)$ does not have a convergent subsequence, contradicting the compactness of $K$. Thus, the first direction is proved. \\
\indent $(\leftarrow)$ Assume that $\range(I - K) = H$. From Lemma 1, $\ker(I - \Ks) = \range(I - K)^{\perp}$. So, $\ker(I - \Ks) = \{ 0 \}$. Therefore, $I - \Ks$ is injective. Using that $\Ks$ is also compact and the forward direction of $(iv)$, we get that $\range(I - \Ks) = H$. Using Lemma 1, $(i)$ for $T = I - K$, we get that 
\[ \ker(I - K) = \range(I - \Ks)^{\perp} = \{ 0\}.\]
\item[(v)] We first show that $\dim(\ker(I - K)) \geq \range(I - K)^{\perp}$. To this end, suppose to the contrary that 
\begin{equation}
\dim \ker(I - K) < \dim \range(I - K)^{\perp}.
\end{equation}
Then, there exists a linear map $A: \ker(I - K) \mapsto \range(I - K)^{\perp}$ which is one-to-one but not onto. We can extend $A$ to a linear map $A : H \mapsto \range(I - K)^{\perp}$ defined on the whole space $H$, by requiring that $Au = 0$ if $u \in \ker(I - K)^{\perp}$. Since the range of $A$ is finite dimensional, the operator $A$ is compact and so is $K + A$. Now we claim that $\ker(I - (K + A)) = \{ 0 \}$. Indeed, consider any vector $u \in H$ and write $u = u_1 + u_2$, where $u_1 \in \ker(I - K)$ and $u_2 \in \ker(I - K)^{\perp}$. then, we see that
\begin{equation}
(I - K - A)(u_1 + u_2) = (I - K) u_2 - A u_1 \in \range(I + K) \oplus \range(I - K)^{\perp}.
\end{equation}
Since $(I - K)u_2$ is orthogonal to $A u_1$, the sum $(I - K)u_2 + A u_1$ cna vanish only if $(I - K) u_2 = 0$ and $A u_1 = 0$. Recalling that the operator $I -K$ is one-to-one on $\ker(I -K)^{\perp}$ and $A$ is one-to-one on $\ker(I - K$, we conclude that $u_1 = u_2 = 0$. Now, applying (iv) to the compact operator $K + A$, we obtain $\range(I - (K + A)) = H$. However, this is impossible: by construction, there exists a vector $v \in \range(I - K)^{\perp}$ with $v \not\in \range(A)$. By $(4)$, the equation
\[ u - Ku - Au = v\]
has no solution. This contradiction shows that (3) cannot hold, and we have that $\dim \ker(I - K) \geq \dim \range(I - K)^{\perp}$. To get the other inequality, we recall that $\range(I - \Ks)^{\perp} = \ker(I - K)$, from the previous step, we deduce that 
\[ \dim \ker(I - \Ks) \geq \dim \range(I - \Ks)^{\perp} = \dim\ker(I - K). \]
Interchanging the roles of $K$ and $\Ks$, we get the opposite inequality and we are done. 
\end{enumerate}
\end{proof}
The big consquence of this theorem is Fredholm Alternative, written out as follows: 
\begin{theorem}(\textbf{Fredholm Alternative:}) Let $K: H \mapsto H$ be a compact operator where $H$ is Hilbert and let $f \in H$. Consider the problem 
\begin{equation}
\textit{Find $u \in H$ such that $u - Ku = f \Longleftrightarrow (I - K)u = f$}.
\end{equation}
Then, one of the following two cases holds (a dichotomy)
\begin{enumerate}
\item[1)] If $\ker(I - K) = \{ 0 \}$, then from $(iv)$, $I - K$ is one to one and onyo, and therefore, $(5)$ has a unique solution, $u = (I - K)^{-1} f$. In addition, there exists a $c > 0$ such that $ \| u \| \leq c \|f \|$. 
\item[2)] If $\ker(I - K) = \spa \{ \phi_1, \cdots, \phi_n \}$ where $\phi_1, \cdots, \phi_n$ is a basis and $n \geq 1$, then $\ker(I - \Ks) = \spa\{ \phi_1, \cdots, \phi_n \}$ which is a basis too. Then, the problem (5) has a solution if and only if
\[ f \in \range(I - K) \Leftrightarrow f \in \ker(I - \Ks)^{\perp} \Leftrightarrow (f, w) = 0  \forall w \in \ker(I - \Ks) \Leftrightarrow (f, \phi_k) = 0 \forall k = 1, \cdots, n.\]
In this case, if $u$ is a solution of $(5)$, then $u + \sum\limits_{ k = 1}^n a_k \phi_k$ is a solution where $a_k \in \R$. 
\end{enumerate}
\end{theorem} 
We now present a couple of notes, or consequences of the Fredholm Alternative:
\begin{enumerate}
\item If $K$ is compact, this implies that $-K$ is compact. Therefore, $I + K$ satisfies the Fredholm Alternative as well.
\item If $A \in \B(H)$ is invertible, then $A^{\star}$ is invertible and $(\As)^{-1} = (A^{-1})^{\star}$. We can see this by showing that since $A \in \B(H)$, then 
\[ A^{-1} A = A^{-1} A = I \Longrightarrow \As(A^{-1})^{\star} = (A^{-1})^{\star} \As = I^{\star} = I.\]
\item Let $A, K \in \B(H)$ be such that $A$ is invertible and $K$ is compact. Then, $A + K$ satisfies the Fredholm Alternative. 
\begin{proof}
First, we note that 
\begin{equation}
A + K = A( I + \Ai K).
\end{equation}
Since $A+ K$ is injective, then, $I + \Ai K$ is injective since $A$ is invertible. Since $\Ai K$ is compact, then $I + \Ai K$ must be surjective. Therefore, since $A$ is surjective, we have that $A(I + \Ai K) =  A + K$ is surjective. \\
\indent Next, from (6), we get that 
\begin{equation}
\As + \Ks = \left(I + (\Ai K)^{\star}\right) \As.
\end{equation}
Then, we see that by the Fredholm Alternative,
\[ \dim \ker(A + K) = \dim \ker( I + \Ai K) = \dim \ker \left(I + (\Ai K)^{\star} \right).\]
From (7), we note that $u \in \ker(\As + \Ks)$ implies that $\As u \in \ker\left(I + (\Ai K)^{\star}\right)$. Therefore, the function that sends $u \to \As u$ from $\ker(\As + \Ks) \mapsto \ker\left(I + (\Ai K)^{\star}\right)$ is one-to-one. Thus, we have that by the Fredholm Alternative
\[ \dim\ker\left(I + (\Ai K)^{\star}\right) = \dim\ker(\As + \Ks)\]
\end{proof}
\item If $A$ is invertible and $K$ is compact, then $(A+ K) u = f$ is solvable if and only if $f \in \ker(\As + \Ks)^{\perp}$
\end{enumerate}
\subsection*{General Spectral Theory}
\setcounter{equation}{0}
Let $X$ be a Banach space over $\K$ ($= \R \text{ or } \C$) and let $T \in \B(X)$. 
\begin{definition}
We say that $\lambda \in \K$ is a
\begin{enumerate}
\item[1)] \textbf{regular point of $T$} if $\lambda I = T$ is invertible.
\item[2)] \textbf{singular point of $T$} if $\lambda I - T$ is not invertible. 
\end{enumerate}
We denote 
\[ \rho(T) = \{ x \in \K : (\lambda I - T)^{-1} \text{ exists } \} = \{ \text{set of all regular points} \}\]
\[ \sigma(T) = \{ x \in \K : (\lambda I - T)^{-1} \text{ does not exist } \} = \{ \text{set of all singular points} \}\]
where $\rho(T)$ is the \textbf{resolvant of $T$} and $\sigma(T)$ is the \textbf{spectrum of $T$}. As well, we say that $R: \rho(T) \to \B(X)$, where $R(\lambda) = (\lambda I - T)^{-1}$ is the \textbf{resolvant function}.
\end{definition}
We briefly note that $\rho(T) \cup \sigma(T) = \K$ and $\rho(T) \cap \sigma(T) = \varnothing$. Now, the spectrum $\sigma(T)$ can be split into three parts, and we define them in the next definition. 
\begin{definition}
Define the following three sets: 
\begin{align*}
\sigma_p(T) & = \{ \lambda \in \sigma(T) : \lambda I - T \text{ is not injective } \} \\
\sigma_c(T) & = \{ \lambda \in \sigma(T) : \lambda I - T \text{ is injective, not surjective, but } \ov{\range(\lambda I - T)} = X \} \\
\sigma_r(T) & = \{ \lambda \in \sigma(T) : \lambda I - T \text{ is injective, but } \ov{\range(\lambda I - T)} \neq X\}. 
\end{align*}
We say that $\sigma_p(T)$ is the \textbf{point spectrum of $T$}, $\sigma_c(T)$ is the \textbf{continuous spectrum of $T$}, and $\sigma_r(T)$ is the \textbf{residual spectrum of $T$}. We note that $\sigma(T) = \sigma_p(T) \cupdot \sigma_c(T) \cupdot \sigma_r(T)$. 
\end{definition}
\begin{definition}
Any $\lambda \in \sigma_p(T)$ is called an \textbf{eigenvalue of $T$}, i.e. there exists an $x \neq 0$ such that $(\lambda I - T) x = 0$, which is also rewritten usually as $Tx = \lambda x$. Any $k \in \ker(\lambda I - T)$, $x \neq 0$ is an eigenvector of $T$ corresponding to $\lambda$. Then, $\ker(\lambda I -T)$ is the \textbf{corresponding eigenspace}. 
\end{definition}
If $X$ is $\fd$, then we know that $\dim \ker(\lambda I - T) + \dim\range(\lambda I - T) = \dim(X) < \infty$. Therefore, we can see that $\ker(\lambda I - T) = \{ 0 \}$ if and only if $\lambda I - T$ is invertible. The negation of this statement is true, i.e. $\ker(\lambda I - T) \neq \{ 0 \}$ if and only if $\lambda I - T$ is not invertible. Therefore, this implies that $\lambda \in \sigma_p(T)$ if and only if $\lambda \in \sigma(T)$. Thus, if $X$ is finite dimensional, we get that $\sigma(T) = \sigma_p(T)$.
\begin{lemma}
Let $S, T \in \B(X)$. Then, 
\begin{enumerate}
\item[(a)] If $\| T \| < 1$, then $I - T$ is invertible and $(I  - T)^{-1} = \sumo T^n = I + T + T^2 + \cdots$, and 
\[ \| (I - T)^{-1} \| < \dfrac{1}{1 - \| T \|}.\]
\item[(b)] If $T$ is invertible and $\| T - S \| < \frac{1}{\| T^{-1} \|}$, then $S$ is invertible and 
\[ \| S^{-1} - T^{-1} \| < \dfrac{\| T^{-1} \|^2 \| S - T \| }{1 - \| T^{-1} \| \| S - T \|}\]
\end{enumerate}
\end{lemma}
\begin{theorem}
Let $X$ be a Banach space and $T \in \B(X)$. then, 
\begin{enumerate}
\item[(a)] $\sigma(T) \sbs \{ \lambda \in \K : |\lambda| \leq \| T \| \}$;
\item[(b)] $\sigma(T)$ is a compact set in $\K$;
\item[(c)] If $\lambda > \| T \|$, then $\lambda \in \rho(T)$ and 
\[ R(\lambda) = \sumo \frac{T^n}{\lambda^{n+1}} \hspace{5mm} \text{ and } \hspace{5mm} \| R(\lambda) \| < \frac{1}{|\lambda| - \| T \|}.\]
\end{enumerate}
\end{theorem}
\begin{proof}
Let $T \in \B(X)$ where $X$ is a Banach space.
\begin{enumerate}
\item[(a)] If $|\lambda | > \| T \|$, then $\lambda I - T = \lambda (I - \frac{1}{\lambda} T)$ and $\| \frac{1}{\lambda} T \| = \frac{1}{|\lambda |} \| T \| < 1$. By Lemma 2, we get that $I - \frac{1}{\lambda} T$ is invertible and therefore, $\lambda I - T$ is invertible. Thus, we must have that $\lambda \in \rho(T)$. Therefore, this all implies that if $|\lambda | > \| T \|$, then $\lambda \in \rho(T)$. Therefore, the contrapositive is true, namely $\lambda \in \sigma(T)$ implies that $|\lambda | \leq \| T \|$, which proves that $\sigma(T)$ is a subset of the set above. 
\item[(b)] From (a), we can see that $\sigma(T)$ is bounded. It is enough then to show that $\sigma(T)$ is closed, or that $\sigma(T)^c = \rho(T)$ is open. To do this, let $\lambda_0 \in \rho(T)$. Then, $\lambda I - T$ is invertible by definition. We want to apply Lemma 2 to this situation. We want to pick a $\lambda$ close to $\lambda_0$ such that if we denote $S = \lambda I - T$ and $T = \lambda_0 I - T$, we can get 
\[ \| S - T \| = \| \lambda - \lambda_0 \| = |\lambda - \lambda_0| < \frac{1}{\| (\lambda_0 I - T)^{-1} \|}.\]
Therefore, select a $\lambda \in \rho(T)$ such that $ |\lambda - \lambda_0| < \frac{1}{\| (\lambda_0 I - T)^{-1} \|}$. Thus, using Lemma 2 part (b), we get that $\lambda I - T$ is invertible, and thus $\lambda \in \rho(T)$. As well, we just proved that $\lambda \in \rho(T)$ implies that $B_{\frac{1}{\| R(\lambda) \|}}(\lambda_0) \sbs \rho(T)$. Therefore, $\rho(T)$ is open and finally, $\sigma(T)$ is closed. 
\item[(c)] We can see that 
\[ R(\lambda) = (\lambda I - T)^{-1} = \frac{1}{\lambda} \left(I - \frac{1}{\lambda}T\right)^{-1}. \]
Now, using Lemma 2(b), we can see that 
\[ \frac{1}{\lambda} \left(I - \frac{1}{\lambda}T\right)^{-1} =  \frac{1}{\lambda} \sumo \frac{T^n}{\lambda^n} = \sumo \frac{T^n}{\lambda^{n+1}}.\]
Using this representation, one can readily see that the norm of $R$ satisfies the condition in the problem. 
\end{enumerate}
\end{proof}
We denote $\sigma_c(T) \cup \sigma_r(T) = \sigma_e(T)$ as the \textbf{essential spectrum}. We now consider properties regarding the spectral properties in a Hilbert space, making this into a proposition.
\begin{proposition}
Let $X$ be a Hilbert space, $\la \cdot, \cdot \ra$ be an inner product and let $\Ts$ be defined as $\la Tx, y \ra = \la x, \Ts y \ra$ for all $x, y \in X$ where $T \in \B(X)$. Then,
\begin{enumerate}
\item[(a)] $\lambda \in \sigma(T)$ if and only if $\ov{\lambda} \in \sigma(\Ts)$;
\item[(b)] If $\lambda \in \sigma_p(T)$, then $\ov{\lambda} \in \sigma_p(\Ts) \cup \sigma_r(\Ts)$;
\item[(c)] If $\lambda \in \sigma_r(T)$, then $\ov{\lambda} \in \sigma_p(\Ts)$.
\end{enumerate}
\end{proposition}
\begin{proof}
\begin{enumerate}
\item[(a)] We will prove that $\lambda \in \rho(T)$ if and only if $\ov{\lambda} \in \rho(\Ts)$ and this will imply our result. We observe the following if and only if statements:
\begin{align*}
\lambda \in \rho(T) & \Longleftrightarrow \lambda I - T \text{ invertible } \\
&\Longleftrightarrow \text{ there exists } S \in \B(X) \text{ such that } (\lambda I - T)S = S(\lambda I - T) = I \\
& \Longleftrightarrow S^{\star}(\lambda I - T)^{\star} = S^{\star} (\ov{\lambda} I - \Ts) = (\ov{\lambda} I - \Ts) S^{\star} = I^{\star} = I \\
& \Longleftrightarrow \ov{\lambda} I - \Ts \text{ invertible } \\
& \Longleftrightarrow \ov{\lambda} \in \rho(\Ts).
\end{align*}
Therefore, taking the negation of the statement, we have our result. 
\item[(b)] If $\lambda \in \sigma_p(T)$, then this implies that $\ker(\lambda I - T) \neq \{ 0 \}$. Thus, we know that $\range(\ov{\lambda} I - \Ts)^{\perp} \neq \{ 0 \}$. Since this is not just the 0 set, we know that the perpendicular set of this cannot be the entire space, or $range(\ov{\lambda} I - \Ts)^{\perp \perp} \neq X$ which can also be written then as $\ov{range(\ov{\lambda} I - \Ts)} \neq X$. Therefore, this must imply that $\ov{\lambda} \not\in \sigma_c(\Ts)$. Therefore, since it is a disjoint union, we must have that $\ov{\lambda} \in \sigma_r(\Ts) \cup \sigma_p(\Ts)$.
\item[(c)] Let $\lambda \in \sigma_r(T)$. Then, we know that $K = \ov{(\lambda I - T) X}$ is a proper subspace of $X$. Therefore, this means that $\{ 0 \} \neq K^{\perp} = \ker(\ov{\lambda} I - \Ts)$, which implies that $\ov{\lambda}$ is an eigenvalue of $\Ts$. Therefore, $\ov{\lambda} \in \sigma_p(\Ts)$.  
\end{enumerate}
\end{proof}
As an example, take $L, R :\ell^2 \mapsto \ell^2$ as the left and right shift operators, namely
\[ L(x_1, x_2, \cdots) = (x_2, x_3, x_4, \cdots) \hspace{5mm} R(x_1, x_2, x_3, \cdots) = (0, x_1, x_2, \cdots).\]
We now want to compute $\sigma_p(L), \sigma(L), \sigma_p(R),$ and $\sigma(R)$. First, we note that $\| L \| = \| R \| = 1$, which has been proved in previous homework assignments. From (a) of Theorem 3, we can see that $\sigma(L), \sigma(R) \sbs \ov{D(0, 1)}$, where $D(0, 1)$ is the disk of radius 1 with center 0. So, let $\lambda \in \sigma_p(L)$. Then, we must have that 
\[ (x_2, x_3, \cdots, x_n \cdots) = (\lambda x_1, \lambda x_2, \cdots, \lambda x_{n -1}, \cdots ).\]
Therefore, if $x_1 = 0$, we then have that $x_i = 0$ for all $i$ and thus, $x = 0$, which cannot happen since $x \neq 0$. So, assume that $x_1 = 1$. Then, $x_2 = \lambda$, $x_3= \lambda^2$, and thus, $x_n = \lambda^{n-1}$ for all $n$. So, $(1, \lambda, \lambda^2, \cdots)$ is an eigenvector provided that $(1, \lambda, \lambda^2, \cdots) \in \ell^2$. Therefore, we need that 
\[ \sumo |\lambda^n|^2 < + \infty, \hspace{4mm} \text{ i.e.,} \hspace{4mm} \sumo |\lambda^2|^n < \infty,\]
which implies that $|\lambda |< 1$. Therefore, all $\lambda \in D(0, 1)$ belongs to $\sigma_p(L)$ and vice versa. So, $\sigma_p(L) = D(0,1)$ and $D(0, 1) = \sigma_p(L) \sbs \sigma(L) \sbs \ov{D(0, 1)}$. Since $\sigma(L)$ is compact, it is closed. Therefore, this must imply that $\sigma(L) = \ov{D(0, 1)}$. We have also seen in our homework that $\sigma(R) = \sigma(L^{\star}) = \ov{\ov{D(0, 1)}} = \ov{D(0,1)}$. Thus, we need to find $\sigma_p(R)$. If $\lambda \in \sigma_p(R)$, then $Rx = \lambda x$ for some $x \neq 0$. We then have 
\[ (0, x_1, x_2, \cdots) = (\lambda x_1, \lambda x_2, \cdots)\]
So, if $\lambda = 0$, then $x = 0$, which is a contradiction. So, if $\lambda \neq 0$, we must have that $x_1 = 0$, which in turn in implies that $x_n = 0$ for all $n$ which again is a contradiction. Therefore, we must have that $\sigma_p(R) = \varnothing$.
\section*{6.2 - Spectrum of Compact Operators}
\begin{theorem}
Let $H$ be an in$\fd$ Hilbert Space and $K: H \mapsto H$ be compact. Then,
\begin{enumerate}
\item[(a)] $0 \in \sigma(K)$;
\item[(b)] $\sigma(K) = \sigma_p(K) \cup \{ 0 \}$ (i.e., the essential supremum is at most 0);
\item[(c)] $\sigma_p(K)$ is either finite or countable and if countable, then $\sigma_p(K) = \{ \lambda_n : n \geq 1\}$ and $\lambda_n \to 0$, i.e. if $\sigma_p(K)$ is infinite, then it accumulates only at 0. 
\end{enumerate}
\end{theorem}
\begin{proof}
\begin{enumerate}
\item[(a)] If $0 \neq \sigma(K)$, then $K$ is invertible. Then, there must exist a $K^{-1} : H \mapsto H$, $K^{-1} \in \B(H)$. So, $I = K \circ K^{-1}$, but since $K$ is compact, $K^{-1}$ is bounded, and compositions of a compact operator and a bounded operator are compact, then $I$ must be compact. Therefore, this implies that $H$ must be finite dimensional, which is a contradiction. 
\item[(b)] We always have that $\sigma_p(K) \sbs \sigma(K)$ and we know from (a) that $0 \in \sigma(K)$. So, $\sigma_p(K) \cup \{ 0 \} \sbs \sigma(K)$. Now, for the other inclusion, it is enough to prove that if $\lambda \in \sigma(K)$ and $\lambda \neq 0$, then $\lambda \in \sigma_p(K)$. Assume to the contrary. Then, $\lambda \not\in \sigma_p(K)$ and thus, $\ker(\lambda I - K) = \{ 0 \}$. Then by the Fredholm Alternative, $\range(\lambda I - K) = H$. Thus, by the Open Mapping Theorem, this implies that $\lambda I - K$ is invertible, which implies that $\lambda \not\in \sigma(K)$, which is a contradiction. Therefore, equality holds.
\item[(c)] To prove this, assume that $(\lambda_n)_{n \geq 1}$ is a sequence of distinct eigenvalues of $K$ where $\lambda_n \to \lambda$. We now claim that $\lambda = 0$. Since $\lambda_n \in \sigma_p(K)$, for each $n \geq 1$, there exists an eigenvector $w_n$ such that $K w_n = \lambda_n w_n$. Denote $H_n : = \spa\{ w_1, \cdots, w_n \}$ and $w_1, \cdots, w_n$ must be linearly independent since $\lambda_1, \cdots, \lambda_n$ are distinct. Therefore, $H_n$ has dimension $n$. We observe that $(K - \lambda_n I) H_n \sbs H_{n-1}$ since 
\[ (K - \lambda_n I) \left( \sumkn \alpha_k w_k \right) = \suml_{k = 1}^{n-1} \alpha_k (K - \lambda_n I) w_k + \underbrace{\alpha_n (K - \lambda_n I) w_n}_{= 0} = \suml_{k = 1}^{n-1} \alpha_k (\lambda_k - \lambda_n)  w_k  \in H. \]
For $n \geq 2$, choose $e_n \in H_{n-1}^{\perp} \cap H_n$ such that $\| e_n \| = 1$. Choose $e_1 \in H_1$ such that $\| e_1 \| = 11$. Then, we have that $(e_n)_{n \geq 1}$ is an orthonormal sequence and it is bounded since $\| e_n \| = 1$. Then we see that assuming $m < n$, then 
\begin{align*}
Ke_n - Ke_m & = \underbrace{(Ke_n - \lambda_n e_n)}_{\in H_{n-1}} -  \underbrace{(K e_m - \lambda_m e_m)}_{\in H_{m -1} \sbs H_{n - 1}} + \lambda_n e_n -  \underbrace{\lambda_m e_m}_{\in H_m \sbs H_{n -1}} \\
& = \lambda_n e_n + z_{n-1},
\end{align*}
where $z_{n - 1} \in H_{n - 1}$. Since $\lambda_n e_n \in H_{n -1}$ as well, we see that 
\[ \| K e_n - K e_m \|^2 = |\lambda_n|^2 + \| z_{n -1}\|^2 \geq |\lambda_n|^2.\]
Therefore, this implies that $\| K e_n - K e_m \| \geq |\lambda_n| \to | \lambda |$. However, if $\lambda \neq 0$, this contradicts $K$ being compact, so $\lambda = 0$> Using a similar argument, we can prove that if $t \geq 0$, then $\{ \lambda \in \sigma_p(K) : |\lambda| > t \}$ is finite. Then,
\[ \sigma_p(K) \sbs \bigcup_{n \geq 1} \{ \lambda \in \sigma_p(K) : |\lambda| \geq \frac{1}{n} \} \cup 0\]
is a countable union of finite sets, and thus countable. Therefore, $\sigma_p(K)$ is at most countable.
\end{enumerate}
\end{proof}
\subsection*{6.3 - Self-Adjoint Operators}
\begin{definition}
Let $H$ be a Hilbert Space and $T: H \mapsto H$ be a bounded linear operator. We define $T$ to be \textbf{$\saj$} (or \textbf{symmetric}) if $\Ts = T$, i.e. $\la Tx, y \ra = \la x, Ty \ra$ for all $x, y \in H$.
\end{definition}
As an example, define $A: \C^2 \mapsto \C^2$, where 
\[ A \left[ \begin{matrix}
z_1 \\
z_2
\end{matrix} \right] = \left[ \begin{matrix}
2 & i \\
-i & 3 
\end{matrix}\right] \left[ \begin{matrix}
z_1 \\
z_2
\end{matrix}\right] \]
One can find that this is $\saj$ since $\As = \ov{A^T} = A$.
\begin{proposition}
Eigenvalues of $\saj$ operators are real. 
\end{proposition}
\begin{proof}
Let $(\lambda, x)$ be an eigenpair for $T = \Ts$. We know that 
\[ \la Tx, x \ra = \la \lambda x, x \ra = \lambda \| x\|^2\]
\[ \la x, Tx \ra = \la x, \lambda x \ra = \ov{\lambda} \| x\|^2.\]
By the $\saj$ness of the operators, we must have that $\ov{\lambda} \| x\|^2 = \lambda \| x \|^2$, which implies $\lambda = \ov{\lambda}$. Therefore, $\lambda \in \R$. 
\end{proof}
\begin{proposition}
If $T$ is $\saj$, then $(Tx, x) \in \R$. 
\end{proposition}
\begin{proof}
We note that $(Tx, x) = \ov{(x, Tx)}$. Since $(x, Tx) = (Tx, x)$, we have that $\ov{(x, Tx)} = (x, Tx)$, which implies that $(x, Tx) \in \R$. Thus, $(Tx, x) \in \R$. 
\end{proof}
Now, we will define 
\begin{align*}
m & = \inf_{\| u \| = 1} (Tu, u)  = \inf_{u \neq 0} \dfrac{(Tu, u)}{\| u \|^2}\\
M & = \sup_{\| u \| = 1} (Tu, u)  = \sup_{u \neq 0} \dfrac{(Tu, u)}{\| u \|^2}
\end{align*}
\begin{lemma} (\textbf{Bounds of the Spectrum of a $\saj$ Operator}
Let $T \in \B(H)$ where $T$ is $\saj$. Then,
\begin{enumerate}[topsep=-15pt, itemsep=0pt]
\item[(i)] $\sigma(T) \sbs [m, M]$
\item[(ii)] $m, M \in \sigma(T)$
\item[(iii)] $\| T \| = \max\{ -m, M \} = \max\{ |m|, |M| \}$
\end{enumerate}
\end{lemma}
\begin{proof}
\begin{enumerate}
\item[(i)] We will prove that if $\lambda \in \R$ and $\lambda \not\in (-\infty, m) \cup (M, \infty)$, then $\lambda \in \rho(T)$. To this end, if $\lambda > M$, then
\[ \la (\lambda I - T) u, u\ra = \lambda \la u, u \ra - \la Tu, u \ra \geq (\lambda - M) \| u \|^2 > 0. \]
By the consequence of the Lax-Milgram theorem, $\lambda I - T$ is invertible and there in $\B(H)$. Similarly, if $\lambda < m$, then $\la (T - \lambda I) u, u \ra \geq (m - \lambda) \| u \|^2$, which means that $T - \lambda I$ is invertible. Thus, $\lambda \in \rho(T)$. Therefore, $\rho(T) \sse (-\infty, m) \cup (M,\infty)$ and therefore, $\sigma(T) \sse [m, M]$. 
\item[(iii)] Without loss of generality, assume $M \geq 0$ and $|m |\leq M$ (if not, replace $T$ by $-T$). Note that for any $u, v \in H$ we have that 
\begin{align*}
4 \la Tu, v \ra = \la T(u + v), u + v \ra - \la T(u - v), u -v \ra & \leq M \left( \| u + v \|^2 + \| u - v \|^2 \right) \\
& = 2 M \left( \| u \|^2 + \| v \|^2 \right).
\end{align*}
Now, replace $v$ by $\frac{\| u \|}{\| Tu \|} Tu$. So, noting that $\| v \| = \| u \|$, we have that 
\[ 2 \| u \| \| T u \| \leq M \left( \| u \|^2  + \| u \|^2 \right)\]
\[ \| T u \| \leq M \| u \|\]
for all $u \in H$, and thus, $ \| T \| \leq M$. Therefore, for all $u \in H$ where $ \| u \| = 1$, we have that 
\[ \la Tu, u \ra \leq \| T u \| \| u \| \leq \| T \| \| u \| \| u \| = \| T \|.\]
Thus, taking the supremum, we have that 
\[ M = \sup\limits_{\| u \| \leq 1} \la Tu, u \ra \leq \| T \|.\]
Thus, $M = \| T \|$. 
\item[(ii)] Without loss of generality, assume that $M \geq 0$ and $|m |< M$, and we claim that $M \in \sigma(T)$. From the definition of $M$, we can choose $(u_n) \sbs H$ such that $\|u_n \| = 1$ and $\la  T u_n , u_n \ra \to M$. Then, 
\begin{align*}
\| Tu_n - M u_n \|^2 &= \| Tu_n \|^2 - 2M \la Tu_n , u_n \ra + M^2 \| u_n \|^2 \\
& \leq \underbrace{\| T \|^2}_{= M^2} \| u_n \|^2 - 2M \underbrace{\la Tu_n, u_n\ra}_{\to M} + M^2 \to 0. 
\end{align*}
Therefore, $\| (MI - T)u_n \| \to 0$ which implies that $(MI - T)u_n \to 0$, and thus, $M \in \sigma(T)$. 
\end{enumerate}
\end{proof}
A couple of notes regarding this lemma:
\begin{enumerate}
\item[(1)] If $T \in \B(H)$ and $T = \Ts$, then 
\[ \| T \| = \sup\limits_{\| x \| = 1 } |\la Tx, x \ra | = \sup\limits_{x \neq 0} \frac{|\la Tx, x \ra |}{\| x \|^2}. \] We note that if $M > |m|$, then $\| T \| = M$ and if $M < |m|$, then $\| T \| = |m|$.
\item[(2)] If $M \geq 0$ and $m = 0$, then $\| T \| = M = \sup\limits_{x \neq 0} \frac{|\la Tx, x \ra|}{\| x \|^2}$. 
\item[(3)] If $ T = \Ts$, $T \in \B(H)$, and $\sigma(T) = \{ 0 \}$, then (iii) tells us that $\| T \| = 0$, which implies that $T \equiv 0$. 
\end{enumerate}
\begin{theorem} (\textbf{Hilbert-Schmidt Theorem})
Eigenvectors of compact symmetric operators form an orthonormal basis. More formally, let $H$ be a separable Hilbert space and $T: H \mapsto H$ be a compact self-adjoint operator. Then, there exists a countable orthonormal basis consisting of eigenvectors of $T$. 
\end{theorem}
\begin{proof}
We remember that if $H$ is finite dimensional and $\Ts T = T \Ts$ (which is clearly satisfied if $T$ is self-adjoint), we say that $T$ is \textbf{normal}. If $\K = \C$, then we have the Compact Spectral Theorem (in Axler), and we know that there exists a finite basis of orthonormal vectors. If $\K = \R$, we have that the Real Spectral Theorem gives us the same result. \\
\indent So, we will assume that $H$ is in$\fd$. Since $T$ is compact, let $(\lambda_n)_{n \geq 1}$ be the sequence of distinct eigenvalues where $\lambda_n \neq 0$ for all $n \geq 1$. Denote $\lambda_0 = 0$, $E_0 = \ker(T)$, and $E_n = \ker(\lambda_n I - T)$. We know that $E_0 \sbs H$ and since $H$ is separable, $E_0$ is separable. Because of this fact, $E_0$ admits an orthonormal countable basis of eigenvectors corresponding to $\lambda_0 = 0$. Since $T$ is compact, we have that $E_n$ is $\fd$ and admits an orthonormal basis. \\
We now claim that $H = E_0 \oplus E_1 \oplus E_2 \cdots \oplus E_n \oplus \cdots$ in the sense that 
\begin{enumerate}
\item[(i)] the spaces $E_n$ are mutually orthogonal, and
\item[(ii)] if $F = \Span\left( \bigcup\limits_{n = 0}^{\infty} E_n \right)$, then $F$ is dense in $H$. 
\end{enumerate}
To show (i), if $u \in E_n$ and $v \in E_m$, where $n \neq m$, we have that 
\[ \lambda_n \la u, v \ra = \la \lambda_n u, v \ra = \la Tu, v \ra = \la  = \la u, Tv \ra = \la u, \lambda_m v \ra = \lambda_m \la u, v \ra\]
This gives us $(\lambda_n - \lambda_m) \la u, v \ra = 0$ and since the eigenvalues are distinct, this necessarily implies that $\la u, v \ra = 0$ and thus, all $E_n$'s are orthogonal to each other. \\
\indent For (ii), we can show that $F$ and $F^{\perp}$ are invariant subspaces for $T$, i.e. $T(F) \sbs F$ and $T(F^{\perp}) \sbs F^{\perp}$. To show this, first let $u\in F = \cup E_n$. Therefore, $u \in E_m$ for some $m \in \N \cup \{ 0\}$. Therefore, $Tu = \lambda_m u$ and since $E_m$ is a subspace, $\lambda_m u \in E_m$. Therefore, $F$ is invariant. To show that $F^{\perp}$ is invariant, let $u \in F^{\perp}$, which implies that $u \perp E_n$ for all $n \geq 0$. Thus, for all $v \in F$, we have that 
\[ \la Tu, v \ra = \la \underbrace{u}_{\in F^{\perp}}, \underbrace{Tv}_{\in F} \ra = 0\]
since $F^{\perp}$ and $F$ are, by definition, orthogonal. Therefore, $Tu \in F^{\perp}$ since it is orthogonal to all vectors $v \in F$. Thus $F^{\perp}$ is invariant under $T$. Now, take $T_0 = T \vert_{F^{\perp}}$. Since $F^{\perp}$ is a closed subspace of $H$, a Hilbert space, $F^{\perp}$ is also a Hilbert space. Since $T$ is compact, $T_0$ is compact and since $T$ is $\saj$, $T_0$ is $\saj$ since 
\[ \la T_0 x, y \ra = \la Tx , y \ra = \la x, Ty \ra = \la x, T_0 y \ra\]
for all $x, y \in F^{\perp}$. We now claim that $\sigma(T_0) = \{ 0 \}$. To this end, suppose not. Then, since $T_0$ is compact, there must exist a $\lambda \in \sigma(T_0)$, $\lambda \neq 0$ such that $\lambda$ is an eigenvalue, i.e. $T_0 v = \lambda v$ for some vector $v \in F^{\perp}$, $v \neq 0$. This also implies that $Tv = \lambda v$ for some $v \neq 0$, and therefore, there must exist an $n \geq 1$ such that $\lambda = \lambda_n$. However, this implies that $v \in E_n \sbs F$, which means that $v \in F \cap F^{\perp}$. Thus, $v = 0$ which is a contradiction. Therefore, $\sigma(T_0) = \{ 0 \}$, which  means that by note (3), $T_0 \equiv 0$. \\
\indent This gives us that $T \vert_{F^{\perp}} \equiv 0$, which necessarily implies that $F^{\perp} \sbs E_0 \sbs F$. Thus, we must have that $F^{\perp} = \{ 0 \}$, which gives us that $F$ is dense in $H$. Therefore, (ii) is proved and thus, we can choose an orthonormal basis for each $E_n$ and union them all together to get a countable orthonormal basis. 
\end{proof}
We now assume that $H$ is a Hilbert and separable space and $T \in \B(H)$ is compact and $\saj$. Let $(e_k)_{k \geq 1}$ be a basis for $\bigcup\limits_{n \geq 1} E_n$, where $E_n = \ker(\lambda_n I - T)$, $E_0 = \ker(T)$ and $(\lambda_n)_{n \geq 1} = \sigma_p(T)$, $\lambda_n \neq 0$. Then, for all $f \in H$, we have that 
\[f = \underbrace{P_{E_0}(f)}_{\in E_0} + \underbrace{( I - P_{E_0})(f)}_{\in E_0^{\perp}} = P_{E_0}(f) + \sum\limits_{k = 1}^{\infty} \la f, e_k \ra e_k.\]
If we assume further that for some $m \geq 0$, $\la Tx, x \ra \geq m\la x, x \ra$ and we note that $\la Tx, x \ra \leq \| T \| \| x \|^2$, this implies that $\| Tx \| \geq M \| x\|$ for all $x \in H$ and thus, $\range(T)$ is closed. Therefore, $\range(T) = \ker(T)^{\perp}$ (since $T$ is $\saj$). Thus, if $f \in \range(T)$, we have that $P_{E_0}(f) = 0$ and thus, 
\[ f = \sum\limits_{k = 1}^{\infty} \la f, e_k \ra e_k, \]
noting that the limits start at 1, and not 0. Why is this helpful? Well, assume we wanted to solve $Tx = f$. Then, we can let 
\[ x = \sum\limits_{k = 1}^{\infty} c_k e_k + P_{E_0} (x)\]
where $ P_{E_0} (x)$ is in the kernel of $T$. Therefore, 
\[ Tx = \sum\limits_{k = 1}^{\infty} c_k T e_k = \sum\limits_{k = 1}^{\infty} \lambda_k c_k e_k.\]
Thus, equating coefficients, we have that $\lambda_k c_k = \la f, e_k \ra$, or 
\[ c_k = \frac{\la f, e_k \ra}{\lambda_k}\]
for all $k \geq 1$. This gives us a representation for $x$ that is easily found via computation.
\section*{Extra Class: Applications of Functional Analysis}
 \setcounter{theorem}{0}
 \setcounter{proposition}{0}
  \setcounter{definition}{0}
 \setcounter{corollary}{0}
 \setcounter{cons}{0}
 \setcounter{equation}{0}
 \setcounter{lemma}{0}
\subsection*{The Rellick-Kondrachor Theorem}
Let $\Om \sbs \R^d$ be an open and bounded set and $D(\Om) = \{ f: \Om \mapsto \R : \supp(f) \text{ is compact }, f \in C^{\infty} \}$ be the set of smooth functions, where $\supp(f) = \ov{\{ x \in \Om : f(x) \neq 0 \}}$. In this section, if we see $(\cdot, \cdot)$, assume that it is $(\cdot, \cdot)_{\ml^2(\Om)}$ unless otherwise noted. 
\begin{theorem}(\textbf{Poincar\'e Inequality}
For any $u \in D(\Om)$, we have that 
\[ \left( \dintom |u|^2 \, dx \right)^{\frac{1}{2}} \leq C(\Om) \left( \dintom |\nabla u|^2 \, dx \right)^{\frac{1}{2}},\]
where $C(\Om)$ is a constant that depends on $\Om$.
\end{theorem}
We will eventually see that $\left( D(\Om), \| \nabla \cdot \|_{\ml^2} \right) \sbs \ell^2$ is a normed space, but it is not complete. Now, we define $\Hz$ as the closure of $\left( D(\Om), \| \nabla \cdot \|_{\ml^2(\Om)} \right)$ in $\ml^2(\Om)$, i.e. 
\[ \Hz(\Om) = \left\lbrace u \in \ml^2(\Om) : \dintom |\nabla u|^2\, dx < \infty, u \vert_{\p \Om} = 0 \right \rbrace. \]
We note that the inner produce on $\Hz(\Om)$ is 
\[ (u, v)_{\Hz} = (\nabla u, \nabla v),\hspace{5mm} \| u \|_{\Hz} =\left( \dintom | \nabla u|^2 \, dx \right)^{\frac{1}{2}}. \]
The Poincar\'e inequality in $\Hz$ is 
\[ \| u \|_{\ml^2(\Om)} \leq C(\Om) \| u \|_{\Hzo}, \hspace{5mm} \forall u \in \Hzo.\]
Therefore, we have that $(\Hzo, \| \cdot \|_{\Hzo}) \sbs \ml^2(\Om)$ is an embedding, and we can see that convergence in $\Hzo$ implies convergence in $\ml^2(\Om)$, but not vice versa. 
\begin{theorem}(\textbf{Rellick-Kondrachor Theorem}) Define $i: \Hzo \to \mlto$, where $iu = u$ for all $u \in \Hz$. Then, $i$ is a compact operator. This implies that bounded sequences in $\Hzo$ have convergent subsequences.
\end{theorem}
This theorem means that $\Hzo$ is a \textbf{compact embedding} into $\mlto$, denoted by $\Hzo \sbs \sbs \mlto$. Define $\is$ to be the Hilbert dual of $i$. Then, $(\is f, v)_{\Hz} = (f, iv) = (f, v)$ for all $v \in H$. Then, $(\nabla u, \nabla v) = (f, v)$ for all $v \in \Hzo$. Then, $\is f$ is the representation of $v \mapsto (f, v)$ in $\Hz$, and thus, 
\[ |(f, v)| \leq \underbrace{\| f \| \| v \|_{\mlt} \leq \| f \|  \, C(\Om)  \, \| v \|_{\Hz}}_{\text{by Poincar\'e Identity}}. \]
\subsection*{Elliptic Equations (Ch. 9 in AB)}
Given $\aij(x)$, $\bi(x)$, $c(x) : \Om \sbs \R^d \mapsto \R$, define 
\begin{equation}
 Lu = - \sumijd (\aij(x) u_{x_i})_{x_j} + \sumid (\bi(x) u )_{x_i} + c(x) u
 \end{equation}
where $u_{x_j} = \frac{\p u}{\p x_j}$. Now, the main problem with these elliptic equations is the following: find $u \in \Hzo$ such that $Lu = f$ given $f \in \mlto$. If $d = 3$, the first summation in (1) is diffusion, the second summation is advection, and the last term is the decay term. We can see the answer $u = u(x)$ as the density of the chemical dispersed within a fluid occupying $\Om$, where $f$ is the source of the chemical. \\
\indent For the remaining part of this section, we take some assumptions. Assume that $\bi = 0$, $c(x) = 0$ and 
\[ \sumijd \aij(x) \xi_i \xi_j \leq \theta \left( |\xi_1|^2 + \cdots + |\xi_n|^2 \right)\]
for all $x \in \Om$. Then, we have our \textbf{uniform} \textbf{ellipticity} condition, 
\[ (A \xi, \xi) \geq \theta |\xi|^2.\]
We also assume that that $a^{ij}$ is symmetric, i.e. $\aij(x) = a^{ji}(x)$ for all $x \in \Om$. Now, we introduce the main problem:
\begin{enumerate}
\item[(MP)] \textit{Given that $f \in \mlto$, find $u \in \Hzo$ such that }
\begin{equation}
Lu  =  - \sumijd (\aij(x) u_{x_i} )_{x_j} = f.
\end{equation}
\end{enumerate}
Trying to solve this question, multiplying (2) by $v \in \Hzo$ and integrating both sides, we have that 
\[ - \sumijd \dintom (\aij(x) u_{x_i})_{x_j} v = \dintom fv,\]
and integrating by parts, we can get that 
\[\dintom (\aij(x) u_{x_i})_{x_j} v  = \dintom \aij(x) u_{x_i} v_{x_j} + \underbrace{\dint_{\p \Omega} (\aij(x) u_{x_j}){x_i} v}_{ = 0}. \]
Therefore, we can find the \textit{weak formulation} for the main problem, (MP).
 \begin{enumerate}
\item[(WF)] \textit{Find $u \in \Hzo$ such that}
\[ B(u, v) = \sumijd \dintom \aij (x) u_{x_i} v_{x_j} = \dintom fv, \hspace{5mm} \forall x \in \Hzo\]
\textit{or $B(u, v) = (f, v)_{\mlto}$ for all $x \in \Hzo$.}
\end{enumerate}
The goal is to now show that (WF) has a unique solution and therefore, this will imply that (MP) has a unique solution. To this end, using that $\aij \in \ml^{\infty}(\Om)$,  
\[ B(u, v) \leq c \| u \|_{\Hzo} \| v \|_{\Hzo}, \hspace{5mm} \forall u, v \in \Hzo,\]
and uniform ellipticity, we have that $B(u, v) \geq \theta \| u \|_{\Hzo}$ for all $u \in \Hzo$. Now, define 
\[F(v) = \dintom fv = (f,v) \]
which is a bounded functional on $\Hz$, i.e. this belongs to $\Hzod$. Then, we can see by the Poincar\'e Inequality that 
\begin{equation}
|(f, v)| = |F(v)| = \left| \dintom fv \right| \leq \| f \| \| v \|_{\mlto} \leq \| f \| \,  C(\Om) \, \| v \|_{\Hzo}.
\end{equation}
Then, using the Riesz Representation Theorem, there must exist a $u_f \in \Hzo$ such that $(u_f, v)_{\Hzo} = F(v)$, which implies that $(\nabla u_f, \nabla v) = (f, v)$ for all $v \in \Hzo$. As well, by using (3), we can have that 
\[ \| u_f \| = \| F \|_{\Hzod} \leq |C(\Om)| \| f \|_{\mlto}.\]
Therefore, (WF) is equivalently asking the following question:
\begin{center}
\textit{Find $u \in \Hzo$ such that $B(u, v) = (u_f, v)_{\Hzo} = (f, v)$ for all $v \in \Hzo$}.
\end{center}
We have now satisfied all of the hypotheses of Lax-Milgram and thus, by that Lax-Milgram theorem, there exists a unique $u \in \Hz$ such that $B(u, v) = (u_f, v) = (f, v)$ for all $v \in \Hz$. As well, we have that 
\[ \| u \|_{\Hzo} \leq \frac{1}{\theta} \| u_f \|_{\Hzo} \leq \frac{C(\Om)}{\theta} \| f \|_{\mlto}.\]
Now, we define $T: \mlto \mapsto \mlto$, $Tf = u$, the solution of the (WF) problem. This is a continuous function, and we can take 
\[ \underbrace{f \mapsto u_f}_{\text{compact}} = \underbrace{\is f \mapsto u = Tf}_{\text{continuous} }.\]
Since this is a compact operator composed with a continuous function, we can then claim that $T$ is compact. As well, since $\aij$ is symmetric, we have that $T$ is $\saj$, i.e.
\[ (Tf, g) = (f, Tg) \hspace{5mm} \forall f, g \in \mlto.\]
We claim that $T$ is injective as well. To this end, assume that $Tf = u = 0$ for some $x \in \Hzo$. Then, we have that 
\[ 0 = B(u, v) = (f, v) \hspace{3mm} \forall v \in \Hzo \]
which implies that 
\[ \dintom fv = 0 \hspace{3mm} \forall v \in \Hzo. \]
Using the fact that $\Hzo$ is dense in $\mlto$, we must have that $f \equiv 0$ and we have proven injectivity. \\
\indent Now, $(u, f) = (Tf, f) = B(u, u)$ since by definition $B(u, v) = (f, v)$ for all $v \in \Hzo$. Thus, taking $v = u$, we have $B(u, u) = (f, u) = (f, Tf) = (Tf, f)$. Thus, 
\[ (Tf, f) = B(u, u) > c \| u \|_{\Hzo}^2 > 0.\]
Thus, by the Hilbert-Schmidt Theorem, there exists $(\lambda_k)_{k \geq 1} \sbs (0, \infty)$ that are eigenvalues, $\lambda_k \to 0$ and there also exist $\phi_k \in \mlto$ such that $T \phi_k = \lambda_k \phi_k$, i.e. $L \phi_k = \lambda_k^{-1} \phi_k$. Therefore, $(\phi_k)_{k \geq 1}$ is an orthonormal basis for $\mlto$ and thus, 
\[ f = \sumk (f, \phi_k) \phi_k\]
for all $f \in \mlto$. Therefore, we have that 
\begin{equation}
u = Tf = \sumk \lambda_k (f, \phi_k) \phi_k,
\end{equation}
which gives us the solution to the (MP)! Therefore, for any type of these problems, we just need to find $\phi_k$ and $\lambda_k$ and we have the answer (this is not as easy as it sounds, but it is still very feasible in most cases).\\
\indent We now present an example of how to use this. Let $\Om = (0, \pi)$ and $Lu = -u_{xx} = - u^{\prime\prime}$. Then, we want to solve
\[Lu = -u^{\pp} (x) = f(x), \hspace{5mm} x \in \Om \]
\[ u(0) = u(\pi) = 0\]
where $f \in \mlto$. Now, to do this, we must just solve for 
\[ Lu = \frac{1}{\lambda^2} u\]
\[- u^{\pp} = \frac{1}{\lambda^2} u.\]
However, the answer to this ODE is just sines and cosines. Via calculations, one can find that $\lambda = k$ and therefore, $\lambda_k = \frac{1}{k^2}$ for $k \geq 1$ and $\phi_k(x) = c_k \sin(kx)$. Needing that $\| \phi_k \| = 1$, we have that $c_k = \sqrt{\frac{2}{\pi}}$. Thus, $\phi_k(x) = \sqrt{\frac{2}{\pi}} \sin(kx)$. Thus, we have found our eigenvalues and eigenfunctions, and we have that $\lambda_k \to 0$ as $k \to \infty$. Therefore, we have that 
\begin{align*}
u = Tf & = \sumk \lambda_k (f \phi_k) \phi_k \\
& = \sumk \frac{1}{k^2} \left( \dintopi f(x) \frac{2}{\sqrt{\pi}} \sin(kx) \, dx\right) \sqrt{\frac{2}{\pi}} \sin(kx) \\
& = \frac{2}{\pi} \sumk \underbrace{\left( \frac{1}{k^2} \dintopi f(x) \, sin(kx) \, dx \right)}_{\text{Fourier Coefficients}} \sin(kx).
\end{align*}
\end{document}
